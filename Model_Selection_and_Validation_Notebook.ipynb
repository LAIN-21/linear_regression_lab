{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LhEMC6GeZxqL"
      },
      "source": [
        "# Model Selection and Validation Notebook\n",
        "\n",
        "In this notebook you will see the code of the Model Selection and Model Validation sessions.\n",
        "\n",
        "We will use the swiss dataset for explaining this variable selection process. We can import it from a URL as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IJbJIV1RZtKH"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import statsmodels.api as sm\n",
        "import statsmodels.formula.api as smf\n",
        "\n",
        "data_url = \"https://vincentarelbundock.github.io/Rdatasets/csv/datasets/swiss.csv\"\n",
        "swiss = pd.read_csv(data_url, index_col=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "id": "oPhchn0DZbBI",
        "outputId": "0b10ef9b-c005-406d-9eed-8e8918ce1763"
      },
      "outputs": [],
      "source": [
        "swiss.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BnuXIQV2Tq4N"
      },
      "source": [
        "In previous sessions we have seen the Backwards stepwise selection manually. We can start with the full model and check which variable to eliminate using the p-values of the beta parameters. Let's start creating the full model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OPpC902MEPIc"
      },
      "outputs": [],
      "source": [
        "swiss[\"Infant_Mortality\"] = swiss[\"Infant.Mortality\"]\n",
        "swiss.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GlExv28BaTUQ",
        "outputId": "d74eca32-a8a5-43f6-cdc7-31d6db00acf0"
      },
      "outputs": [],
      "source": [
        "model_1 = smf.ols(formula = \"Fertility ~ Agriculture + Examination + Education + Catholic + Infant_Mortality\",\n",
        "                  data = swiss).fit()\n",
        "print(model_1.summary())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xAPv-hYLT4ib"
      },
      "source": [
        "We see that the variable with the greater p-value is \"Examination\", so we will create a `model_2` eliminating that variable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KJ6D8cX_dTeY",
        "outputId": "f6a4a2be-5df2-4efa-c1bb-d78662b26a13"
      },
      "outputs": [],
      "source": [
        "model_2 = smf.ols(formula = \"Fertility ~ Agriculture + Education + Catholic + Infant_Mortality\",\n",
        "                  data = swiss).fit()\n",
        "print(model_2.summary())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZrnjjF7kUESF"
      },
      "source": [
        "This could be a good model for us just using the variable selection strategy that we have studied so far. In this notebook we will see several strategies for variable selection, so we can compare them and take better informed decisions.\n",
        "\n",
        "## ANOVA method\n",
        "\n",
        "We will compare different models using ANOVA depending on the difference on their F statistic result."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7XHsyKe5fM7G",
        "outputId": "2ae1560f-4387-4b7f-8acf-0f6a8926f9a7"
      },
      "outputs": [],
      "source": [
        "model_0 = smf.ols(formula = \"Fertility ~ 1\", data = swiss).fit()\n",
        "print(model_0.summary())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 112
        },
        "id": "CHofF88GeScY",
        "outputId": "1a005f5d-adb1-479e-a5fc-f4f63f201ae4"
      },
      "outputs": [],
      "source": [
        "from statsmodels.stats.anova import anova_lm\n",
        "anova_lm(model_0, model_1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 112
        },
        "id": "AGQXorKvhDev",
        "outputId": "2fb340c2-932e-41cb-bad4-f4aa31ad2c17"
      },
      "outputs": [],
      "source": [
        "anova_lm(model_2, model_1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g8XRZKuSUgm7"
      },
      "source": [
        "## Best Subset Regression\n",
        "\n",
        "There is not a specific function in Python for obtaining the results of a best subset regression, so we have to create a program for performing those operations (this could be a good programming exercise if you want to try by yourselves). Below, you have my solution for this best subset regression function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MXopWSxh5yy_"
      },
      "outputs": [],
      "source": [
        "import itertools\n",
        "\n",
        "# Function to fit a model and return the adjusted R-squared\n",
        "def fit_model_and_calc_aic(y, X):\n",
        "    X = sm.add_constant(X)\n",
        "    model = sm.OLS(y, X).fit()\n",
        "    return {\"aic\" : model.aic, \"bic\" : model.bic, \"r2\" : model.rsquared_adj}\n",
        "\n",
        "def best_subset_reg(df, y, predictors, metric = \"aic\"):\n",
        "  # Initialize the best subset and its performance\n",
        "  best_subset = []\n",
        "  best_subset_metric = -np.inf\n",
        "  best_subset_metric_aic = np.inf\n",
        "  all_subsets = []\n",
        "  all_models = []\n",
        "\n",
        "  # Iterate over all possible subsets of predictors\n",
        "  for subset_size in range(1, len(predictors) + 1):\n",
        "      for subset in itertools.combinations(predictors, subset_size):\n",
        "          X = df[list(subset)]\n",
        "          model = fit_model_and_calc_aic(y, X)\n",
        "          all_subsets.append(subset)\n",
        "          all_models.append(model)\n",
        "          current_metric = model[metric]\n",
        "\n",
        "          if (metric == \"r2\") and (current_metric > best_subset_metric):\n",
        "              best_subset = subset\n",
        "              best_subset_metric = current_metric\n",
        "          elif (metric != \"r2\") and (current_metric < best_subset_metric_aic):\n",
        "              best_subset = subset\n",
        "              best_subset_metric_aic = current_metric\n",
        "\n",
        "  print(\"Best subset:\", best_subset)\n",
        "  print(metric, \":\", best_subset_metric_aic)\n",
        "  bestsubreg = pd.DataFrame(all_subsets, all_models)\n",
        "  return(bestsubreg)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BrYRkN3MU2Ya"
      },
      "source": [
        "With this function we can print an specific statistic that we want to evaluate, but all the results will be stored in the return of the function, so we can visualize every step of the algorithm."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TgfNfUgRAj0r",
        "outputId": "8a582031-7c04-48aa-9501-5e49909b0275"
      },
      "outputs": [],
      "source": [
        "# Target variable\n",
        "y = swiss['Fertility']\n",
        "\n",
        "# Define the predictor variables\n",
        "predictors = ['Agriculture', 'Examination', 'Education', 'Catholic', 'Infant_Mortality']\n",
        "\n",
        "# Use the function\n",
        "allsub = best_subset_reg(df = swiss, y = y, predictors = predictors, metric = \"aic\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "NjAk58y9BwZW",
        "outputId": "3f34d90f-d987-4862-ff09-cc93e9797040"
      },
      "outputs": [],
      "source": [
        "allsub"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7bC7V1yfVDDA"
      },
      "source": [
        "## Stepwise method\n",
        "\n",
        "Again, there is not an algorithm in statsmodels that allow us to perform the stepwise selection. We have two options, we can do it by hand or using a different library named “mlxtend”. Both options are used in the notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0TEQv7mjFr8w"
      },
      "outputs": [],
      "source": [
        "def forward_selection(data, target, significance_level=0.05):\n",
        "    initial_features = data.columns.tolist()\n",
        "    best_features = []\n",
        "    while len(initial_features) > 0:\n",
        "        remaining_features = list(set(initial_features) - set(best_features))\n",
        "        new_pval = pd.Series(index=remaining_features, dtype='float64')\n",
        "        for new_column in remaining_features:\n",
        "            model = sm.OLS(target, sm.add_constant(data[best_features + [new_column]])).fit()\n",
        "            new_pval[new_column] = model.pvalues[new_column]\n",
        "        min_p_value = new_pval.min()\n",
        "        if min_p_value < significance_level:\n",
        "            best_features.append(new_pval.idxmin())\n",
        "        else:\n",
        "            break\n",
        "    return best_features\n",
        "\n",
        "def backward_elimination(data, target, significance_level=0.05):\n",
        "    features = data.columns.tolist()\n",
        "    while len(features) > 0:\n",
        "        features_with_constant = sm.add_constant(data[features])\n",
        "        p_values = sm.OLS(target, features_with_constant).fit().pvalues[1:]\n",
        "        max_p_value = p_values.max()\n",
        "        if max_p_value >= significance_level:\n",
        "            excluded_feature = p_values.idxmax()\n",
        "            features.remove(excluded_feature)\n",
        "        else:\n",
        "            break\n",
        "    return features\n",
        "\n",
        "def bidirectional_stepwise(data, target, significance_level=0.05):\n",
        "    forward_features = forward_selection(data, target, significance_level)\n",
        "    return backward_elimination(data[forward_features], target, significance_level)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nbaNu3Y0FuK3",
        "outputId": "772b42c8-8366-44e4-dc96-b11fa0a8f2f1"
      },
      "outputs": [],
      "source": [
        "# Define the target variable and predictors\n",
        "target = swiss['Fertility']\n",
        "predictors = swiss.drop(columns='Fertility')\n",
        "\n",
        "# Perform forward selection\n",
        "forward_selected_features = forward_selection(predictors, target)\n",
        "print(\"Forward Selection: \", forward_selected_features)\n",
        "\n",
        "# Perform backward elimination\n",
        "backward_eliminated_features = backward_elimination(predictors, target)\n",
        "print(\"Backward Elimination: \", backward_eliminated_features)\n",
        "\n",
        "# Perform bidirectional stepwise regression\n",
        "bidirectional_features = bidirectional_stepwise(predictors, target)\n",
        "print(\"Bidirectional Stepwise: \", bidirectional_features)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RvdIWiKBHkj0"
      },
      "outputs": [],
      "source": [
        "swiss[\"Infant_Mortality\"] = swiss[\"Infant.Mortality\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "64EBoWJPFIPG"
      },
      "outputs": [],
      "source": [
        "#!pip install --upgrade mlxtend\n",
        "import joblib\n",
        "import sys\n",
        "sys.modules['sklearn.externals.joblib'] = joblib\n",
        "from mlxtend.feature_selection import SequentialFeatureSelector as SFS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "op1UAsr7IttB",
        "outputId": "b5218087-3369-47dd-e104-857e9155fc34"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "#from mlxtend.feature_selection import SequentialFeatureSelector as SFS\n",
        "\n",
        "target = swiss['Fertility']\n",
        "predictors = swiss.drop(columns='Fertility')\n",
        "\n",
        "linear_regression = LinearRegression()\n",
        "\n",
        "# Forward Selection\n",
        "forward_selector = SFS(linear_regression,\n",
        "                       k_features=\"best\",\n",
        "                       forward=True,\n",
        "                       floating=False,\n",
        "                       scoring='r2',\n",
        "                       cv=0)\n",
        "forward_selector.fit(predictors, target)\n",
        "forward_selected_features = list(predictors.columns[list(forward_selector.k_feature_idx_)])\n",
        "print(\"Forward Selection: \", forward_selected_features)\n",
        "\n",
        "# Backward Elimination\n",
        "backward_selector = SFS(linear_regression,\n",
        "                        k_features=\"best\",\n",
        "                        forward=False,\n",
        "                        floating=False,\n",
        "                        scoring='r2',\n",
        "                        cv=0)\n",
        "backward_selector.fit(predictors, target)\n",
        "backward_eliminated_features = list(predictors.columns[list(backward_selector.k_feature_idx_)])\n",
        "print(\"Backward Elimination: \", backward_eliminated_features)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MhJzIlplKLmq",
        "outputId": "d49c1c28-f544-46a3-cb21-d2ac68ec8393"
      },
      "outputs": [],
      "source": [
        "def summarize_results(selector, method, predictors):\n",
        "    selected_features = list(predictors.columns[list(selector.k_feature_idx_)])\n",
        "    print(f\"{method} Results:\")\n",
        "    print(\"Selected features:\", selected_features)\n",
        "    print(\"Number of features:\", selector.k_feature_names_)\n",
        "    print(\"R-squared:\", selector.k_score_)\n",
        "    print(\"\\nFeature Selection History:\")\n",
        "    for idx, values in selector.subsets_.items():\n",
        "        print(\"Step\", idx, \": Features\", list(predictors.columns[list(values[\"feature_idx\"])]), \"- R-squared:\" ,values[\"avg_score\"])\n",
        "\n",
        "# Summarize Forward Selection results\n",
        "summarize_results(forward_selector, \"Forward Selection\", predictors)\n",
        "print(\"\\n\")\n",
        "# Summarize Backward Elimination results\n",
        "summarize_results(backward_selector, \"Backward Elimination\", predictors)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2GZ5yoyXW3w4"
      },
      "source": [
        "# Model Validation\n",
        "\n",
        "Using the best model, now we can obtain the validated scores using the methods we have learned at the beginnig of this module: Holdout (train-test), LOOCV and K-Fold."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8EM8Rd3cXMWW"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split, LeaveOneOut, KFold, cross_val_score\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "target = swiss['Fertility']\n",
        "predictors = swiss.drop(columns=['Fertility', 'Examination']) # we eliminate in this case Examination"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7lrMW3vXXibg"
      },
      "source": [
        "## Holdout"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P4dPQvV8XeVA",
        "outputId": "399f0c27-33e1-491c-809b-fdd59bb10ac9"
      },
      "outputs": [],
      "source": [
        "# Split the data into training and testing sets (70% train, 30% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(predictors, target, test_size=0.3, random_state=2023)\n",
        "\n",
        "# Create a linear regression model and fit it on the training data\n",
        "lr_model = LinearRegression().fit(X_train, y_train)\n",
        "\n",
        "# Calculate the R-squared score on the test data\n",
        "holdout_score = lr_model.score(X_test, y_test)\n",
        "print(f\"Holdout R-squared: {holdout_score}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NBeapmeeX3AH"
      },
      "source": [
        "## LOOCV\n",
        "\n",
        "the R-squared score is not well-defined when there is only one sample in the test set, which is the case with Leave-One-Out Cross-Validation (LOOCV). In this situation, it's better to use a different scoring metric, such as the mean squared error (MSE) or mean absolute error (MAE).\n",
        "\n",
        "But in order to compare it, I have developed a program that manually calculates the R-squared for each iteration of the LOOCV method, giving the final average R-squared."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e22y5QEWX4YZ",
        "outputId": "c926be17-60e0-4db8-9c34-ff6950f1df34"
      },
      "outputs": [],
      "source": [
        "# Create a LeaveOneOut object\n",
        "loo = LeaveOneOut()\n",
        "\n",
        "# Create a linear regression model\n",
        "lr_model = LinearRegression()\n",
        "\n",
        "# Keep track of true and predicted values\n",
        "y_true = []\n",
        "y_pred = []\n",
        "\n",
        "# Iterate through each split in the LOOCV\n",
        "for train_index, test_index in loo.split(predictors):\n",
        "    X_train, X_test = predictors.iloc[train_index], predictors.iloc[test_index]\n",
        "    y_train, y_test = target.iloc[train_index], target.iloc[test_index]\n",
        "\n",
        "    # Fit the model on the training data\n",
        "    lr_model.fit(X_train, y_train)\n",
        "\n",
        "    # Make predictions on the test data\n",
        "    predictions = lr_model.predict(X_test)\n",
        "\n",
        "    # Append the true and predicted values\n",
        "    y_true.extend(y_test)\n",
        "    y_pred.extend(predictions)\n",
        "\n",
        "# Convert the true and predicted values to numpy arrays\n",
        "y_true = np.array(y_true)\n",
        "y_pred = np.array(y_pred)\n",
        "\n",
        "# Compute the R-squared manually\n",
        "loocv_r2 = 1 - ((y_true - y_pred) ** 2).sum() / ((y_true - y_true.mean()) ** 2).sum()\n",
        "print(f\"LOOCV R-squared: {loocv_r2}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EqBrsbXVZMNB"
      },
      "source": [
        "## K-Fold"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hlxx0qMZZQs2",
        "outputId": "bba8478a-7284-4cd9-916f-057f4884d076"
      },
      "outputs": [],
      "source": [
        "# Create a KFold object with 5 folds\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=2023)\n",
        "\n",
        "# Create a linear regression model\n",
        "lr_model = LinearRegression()\n",
        "\n",
        "# Perform k-Fold Cross-Validation and calculate the R-squared score for each fold\n",
        "kfold_scores = cross_val_score(lr_model, predictors, target, cv=kf, scoring='r2')\n",
        "\n",
        "# Calculate the average R-squared score across all k-Fold Cross-Validation iterations\n",
        "kfold_avg_score = kfold_scores.mean()\n",
        "print(f\"k-Fold Average R-squared: {kfold_avg_score}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
